\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{algorithm2e} % pseudo-code
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage[procnames]{listings}

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\definecolor{ans}{rgb}{0,.5,0}%{0.545,0.27,0.074}
\def\ans#1{{\color{ans}#1}}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
\def\norm#1{\|#1\|}

% Configuration
\usepackage[font={color=ans,bf},figurename=Fig.,labelfont={it}]{caption}
\lstset{
    language=Python, 
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{keywords},
    commentstyle=\color{comments},
    stringstyle=\color{red},
    showstringspaces=false,
    identifierstyle=\color{green},
    procnamekeys={def,class}
}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}
\def\cond{\; | \;}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a2f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}


\begin{document}

\title{CPSC 340 Assignment 2 (due 2019-01-25 at 11:55pm)}
\author{}
\date{}
\maketitle
\vspace{-4em}
\ans{
    Student name: José Abraham Torres Juárez \\
    Student id: 79507828 \\ \\
}
\section*{Instructions}
\rubric{mechanics:5}

\textbf{IMPORTANT!!! Before proceeding, please carefully read the general homework instructions at} \url{https://www.cs.ubc.ca/~fwood/CS340/homework/}. The above 5 points are for following the submission instructions. You can ignore the words ``mechanics'', ``reasoning'', etc.

\vspace{1em}
We use \blu{blue} to highlight the deliverables that you must answer/do/submit with the assignment.

\section{Training and Testing}
If you run \texttt{python main.py \string-q 1}, it will load the \emph{citiesSmall.pkl} data set from Assignment 1.
Note that this file contains not only training data, but also test data, \texttt{X\string_test} and \texttt{y\string_test}.
After training a depth-2 decision tree with the information gain splitting rule, it will evaluate the performance of the classifier on the test data.
With a depth-2 decision tree, the training and test error are fairly close, so the model hasn't overfit much.

\subsection{Training and Testing Error Curves}
\rubric{reasoning:2}

\blu{Make a plot that contains the training error and testing error as you vary the depth from 1 through 15. How do each of these errors change with the decision tree depth?}

Note: it's OK to reuse code from Assignment 1.

\fig{1}{../figs/q1_1_errors.pdf}
\ans{
   The training error starts to reach 0 (Overfitting) around depth 8. But the testing error doesn't 
   pass the $0.078\%$ error.
}

\subsection{Validation Set}
\rubric{reasoning:3}

Suppose that we didn't have an explicit test set available. In this case, we might instead use a \emph{validation} set. Split the training set into two equal-sized parts: use the first $n/2$ examples as a training set and the second $n/2$ examples as a validation set (we're assuming that the examples are already in a random order). \blu{What depth of decision tree would we pick to minimize the validation set error? Does the answer change if you switch the training and validation set? How could use more of our data to  estimate the depth more reliably?} \\
\clearpage
\begin{figure}[htp]
   \begin{subfigure}[b]{0.5\textwidth} \color{ans}
       \centerfig{1}{../figs/q1_2_training_validation_1}
       {
         \begin{center}
            $1st$ half as training set and $2nd$ as validation
         \end{center}
       }
     \label{fig:1}
   \end{subfigure}
   %
   \begin{subfigure}[b]{0.5\textwidth} \color{ans}
       \centerfig{1}{../figs/q1_2_training_validation_2}
      {
          \begin{center}
            $2nd$ half as training set and $1st$ as validation
          \end{center}
      }
     \label{fig:2}
   \end{subfigure}
\end{figure}
\ans{
   By looking at the left plot, we can minimize the validation error by choosing either 8 or 9 as 
   depth of the tree. But if we swap the training and validation sets we minimize the validation error 
   by choosing a depth of 6. \\ \\
   To estimate the depth more reliable we can do cross-validation with the entire training set.
}

\section{Naive Bayes}

In this section we'll implement naive Bayes, a very fast classification method that is often surprisingly accurate for text data with simple representations like bag of words.



\subsection{Naive Bayes by Hand}

Consider the dataset below, which has $10$ training examples and $3$ features:
\[
X = \begin{bmatrix}0 & 0 & 1\\0 & 1 & 1\\ 0 & 1 & 1\\ 1 & 1 & 0\\0 & 1 & 0\\0 & 1 & 1\\1 & 0 & 0\\1 & 1 & 0\\1 & 0 & 1\\1 & 0 & 0\\\end{bmatrix}, \quad y = \begin{bmatrix}\text{spam}\\\text{spam}\\\text{spam}\\\text{spam}\\\text{spam}\\\text{spam}\\\text{not spam}\\\text{not spam}\\\text{not spam}\\\text{not spam}\end{bmatrix}.
\]
The feature in the first column is $<$your name$>$ (whether the e-mail contained your name), in the second column is ``pharmaceutical'' (whether the e-mail contained this word), and the third column is ``PayPal'' (whether the e-mail contained this word).
Suppose you believe that a naive Bayes model would be appropriate for this dataset, and you want to classify the following test example:
\[
\hat{x} = \begin{bmatrix}1 & 1 & 0\end{bmatrix}.
\]

\subsubsection{Prior probabilities}
\rubric{reasoning:1}
\blu{Compute the estimates of the class prior probabilities} (you don't need to show any work):
\items{
\item $p(\text{spam})$.
\ans{
   $\frac{6}{10}$ =
   \fbox{
      0.6
   }
}
\item $p(\text{not spam})$.
\ans{
   $\frac{4}{10}$ =
   \fbox{
      0.4
   }
}
}

\subsubsection{Conditional probabilities}
\rubric{reasoning:1}

\blu{Compute the estimates of the 6 conditional probabilities required by naive Bayes for this example}  (you don't need to show any work):
\items{
\item $p(\text{$<$your name$>$} = 1  \cond \text{spam})$.
\ans{
   $\frac{1}{6} \approx$
   \fbox{
      0.16667
   }
}
\item $p(\text{pharmaceutical} = 1 \cond \text{spam})$.
\ans{
   $\frac{5}{6} \approx$
   \fbox{
      0.83333
   }
}
\item $p(\text{PayPal} = 0  \cond \text{spam})$.
\ans{
   $\frac{2}{6} \approx$
   \fbox{
      0.33333
   }
}
\item $p(\text{$<$your name$>$} = 1  \cond \text{not spam})$.
\ans{
   $\frac{4}{4} =$
   \fbox{
      1
   }
}
\item $p(\text{pharmaceutical} = 1  \cond \text{not spam})$.
\ans{
   $\frac{1}{4} =$
   \fbox{
      0.25
   }
}
\item $p(\text{PayPal} = 0  \cond \text{not spam})$.
\ans{
   $\frac{3}{4} =$
   \fbox{
      0.75
   }
}
}

\subsubsection{Prediction}
\rubric{reasoning:1}

\blu{Under the naive Bayes model and your estimates of the above probabilities, what is the most likely label for the test example? (Show your work.)} 
\ans{
   \begin{center}
      $p$(spam $|$ $ \hat{x}$) = $p$($<$your name$>$ = 1, pharmaceutical = 1, PayPal = 0 $|$ $ \hat{y}$ = spam) $p$($\hat{y}$ = spam) \\ = \\ 
      $p$($<$your name$>$ = 1 $| \hat{y}$ = spam) $p$(pharmaceutical = 1 $| \hat{y}$ = spam) $p$(PayPal = 0 $| \hat{y}$ = spam) $p$($\hat{y}$ = spam) \\ = \\
      $(\frac{1}{6}) (\frac{5}{6}) (\frac{2}{6}) (0.6) = \frac{60}{2160} = \frac{1}{36}$
   \end{center}
   \begin{center}
      \fbox{$p$(spam $|$ $ \hat{x}$) = $\frac{1}{36} \approx 0.02778$}
   \end{center}
   \begin{center}
      $p$(not spam $|$ $ \hat{x}$) = $p$($<$your name$>$ = 1, pharmaceutical = 1, PayPal = 0 $|$ $ \hat{y}$ = not spam) $p$($\hat{y}$ = not spam) \\ = \\ 
      $p$($<$your name$>$ = 1 $| \hat{y}$ = not spam) $p$(pharmaceutical = 1 $| \hat{y}$ = not spam) $p$(PayPal = 0 $| \hat{y}$ = not spam) $p$($\hat{y}$ = not spam) \\ = \\
      $(1) (0.25) (0.75) (0.6) = 0.1125$
   \end{center}
   \begin{center}
      \fbox{$p$(not spam $|$ $ \hat{x}$) = $0.1125$}
   \end{center} 
   $p$(not spam $|$ $ \hat{x}$) $>$ $p$(spam $|$ $ \hat{x}$), therefore the most likely label for the 
   test example $\hat{x}$ is "not spam"
}
\subsubsection{Laplace smoothing}
\label{laplace.conceptual}
\rubric{reasoning:2}

One way to think of Laplace smoothing is that you're augmenting the training set with extra counts. Consider the estimates of the conditional probabilities in this dataset when we use Laplace smoothing (with $\beta = 1$). 
\blu{Give a set of extra training examples that we could add to the original training set that would make the basic estimates give us the estimates with Laplace smoothing} (in other words give a set of extra training examples that, if they were included in the training set and we didn't use Laplace smoothing, would give the same estimates of the conditional probabilities as using the original dataset with Laplace smoothing).
Present your answer in a reasonably easy-to-read format, for example the same format as the data set at the start of this question.

\[\color{ans}
X_{laplace} = \begin{bmatrix}
1 & 1 & 1\\
0 & 0 & 0\\
1 & 1 & 1\\
0 & 0 & 0\\
\end{bmatrix}, \quad y_{laplace} = 
\begin{bmatrix}
\text{spam}\\
\text{spam}\\
\text{not spam}\\
\text{not spam}
\end{bmatrix}.
\]

\subsection{Bag of Words}
\rubric{reasoning:3}

If you run \texttt{python main.py -q 2.2}, it will load the following dataset:
\enum{
\item $X$: A binary matrix. Each row corresponds to a newsgroup post, and each column corresponds to whether a particular word was used in the post. A value of $1$ means that the word occured in the post.
\item $wordlist$: The set of words that correspond to each column.
\item $y$: A vector with values $0$ through $3$, with the value corresponding to the newsgroup that the post came from.
\item $groupnames$: The names of the four newsgroups.
\item $Xvalidate$ and $yvalidate$: the word lists and newsgroup labels for additional newsgroup posts.
}
\blu{Answer the following}:
\enum{
\item Which word corresponds to column 51 of $X$? (This is column 50 in Python.) \\
\ans{The word is \fbox{lunar}}
\item Which words are present in training example 501? \\
\ans{The example 501 has the words: \fbox{'car' 'fact' 'gun' 'video'}}
\item Which newsgroup name does training example 501 come from? \\
\ans{The groupname is: \fbox{talk.*}}
}

\subsection{Naive Bayes Implementation}
\rubric{code:5}

If you run \texttt{python main.py -q 2.3}
it will load the newsgroups dataset, fit a basic naive Bayes model and report the validation error.

The \texttt{predict()} function of the naive Bayes classifier is already implemented.
However, in \texttt{fit()}
the calculation of the variable \texttt{p\_xy} is incorrect
(right now, it just sets all values to $1/2$).
\blu{Modify this function so that \texttt{p\_xy} correctly
computes the conditional probabilities of these values based on the
frequencies in the data set. Submit your code and the validation error that you obtain.
Also, compare your validation error to what you obtain with scikit-learn's implementation, \texttt{BernoulliNB}.} \\ \\
\ans{
   The Naive Bayes classifier implemented has a validation error of $0.188$, the Naive Bayes classifier 
   from scikit-learn (\texttt{BernoulliNB}) has a validation error of $0.187$. \\ \\
   See the following code for the implementation of \texttt{Naive Bayes fit()}.\\
   \begin{center}
      \lstinputlisting[firstline=11,lastline=34]{../code/naive_bayes.py}
  \end{center}
}

\subsection{Runtime of Naive Bayes for Discrete Data}
\rubric{reasoning:3}

For a given training example $i$, the predict function in the provided code computes the quantity
\[
p(y_i \cond x_i) \propto p(y_i)\prod_{j=1}^d p(x_{ij} \cond y_i),
\]
for each class $y_i$ (and where the proportionality constant is not relevant). For many problems, a lot of the $p(x_{ij} \cond y_i)$ values may be very small. This can cause the above product to underflow. The standard fix for this is to compute the logarithm of this quantity and use that $\log(ab) = \log(a)+\log(b)$,
\[
\log p(y_i \cond x_i) = \log p(y_i) + \sum_{j=1}^d \log p(x_{ij} \cond y_i) + \text{(irrelevant propportionality constant)} \, .
\]
This turns the multiplications into additions and thus typically would not underflow.

Assume you have the following setup:
\items{
\item The training set has $n$ objects each with $d$ features.
\item The test set has $t$ objects with $d$ features.
\item Each feature can have up to $c$ discrete values (you can assume $c \leq n$).
\item There are $k$ class labels (you can assume $k \leq n$)
}
You can implement the training phase of a naive Bayes classifier in this setup in $O(nd)$, since you only need to do a constant amount of work for each $X(i,j)$ value. (You do not have to actually implement it in this way for the previous question, but you should think about how this could be done.)
 \blu{What is the cost of classifying $t$ test examples with the model and this way of computing the predictions?} \\ \\
% JATJ: check
\ans{
   The cost of classifying $t$ test examples is $O(tdck)$, because for each test sample we have to get
   $p(x_{ij}$ which will be done in $d$ time and then $c$ to calculate the feature with all of its 
   possible values, finally we will repeat this process $k$ times for the number of classes. 
   So the whole cost of the prediction will be $O(tdck)$.
}

\section{K-Nearest Neighbours}
\rubric{code:3, reasoning:4}

In the \emph{citiesSmall} dataset, nearby points tend to receive the same class label because they are part of the same U.S. state. For this problem, perhaps a $k$-nearest neighbours classifier might be a better choice than a decision tree. The file \emph{knn.py} has implemented the training function for a $k$-nearest neighbour classifier (which is to just memorize the data).


Fill in the \texttt{predict} function in \texttt{knn.py} so that the model file implements the $k$-nearest neighbour prediction rule.
You should Euclidean distance, and may numpy's \texttt{sort} and/or \texttt{argsort} functions useful.
You can also use \texttt{utils.euclidean\string_dist\string_squared}, which computes the squared Euclidean distances between all pairs of points in two matrices.
\blu{
\enum{
\item Write the \texttt{predict} function. \\
\ans{
   See the following code for the implementation of \texttt{K-Nearest Neighbours predict()}.
   \begin{center}
      \lstinputlisting[firstline=17,lastline=28]{../code/knn.py}
  \end{center}
}
\item Report  the training and test error obtained on the \emph{citiesSmall} dataset for $k=1$, $k=3$, and $k=10$. How do these numbers compare to what you got with the decision tree? \\
\ans{
   \scriptsize
   \setlength{\tabcolsep}{2pt}
   \begin{center}
   \begin{tabular}{SSSS} \toprule
      {} & {$k = 1$} & {$k = 3$} & {$k = 10$} \\ \midrule
      {Training error (ours)} & 0.000 & 0.028 & 0.072 \\ 
      {Test error (ours)} & 0.065 & 0.066 & 0.097 \\ \midrule
      {Training error (scikit)} & 0.000 & 0.028 & 0.072 \\ 
      {Test error (scikit)} & 0.065 & 0.066 & 0.097 \\ \bottomrule
   \end{tabular}
   \end{center}
   \normalsize
   The raining and testing errors using KNN are much smaller than with the decision tree
}
\item Hand in the plot generated by \texttt{utils.plotClassifier} on the \emph{citiesSmall} dataset for $k=1$, using both your implementation of KNN and the KNeighborsClassifier from scikit-learn.
\begin{figure}[htp]
   \begin{subfigure}[b]{0.5\textwidth} \color{ans}
       \centerfig{1}{../figs/q3_3_myKNN}
       {
          \begin{center}
            My implementation of KNN
          \end{center}
      }
     \label{fig:1}
   \end{subfigure}
   %
   \begin{subfigure}[b]{0.5\textwidth} \color{ans}
       \centerfig{1}{../figs/q3_3_scikitKNN}
      {
          \begin{center}
            Scikit-Learn KNN
          \end{center}
      }
     \label{fig:2}
   \end{subfigure}
\end{figure}
\clearpage
\item Why is the training error $0$ for $k=1$? \\
\ans{
   Because the \texttt{predict()} function is giving the exact values of the closest neighbour and not 
   averaging between several neighbors.
}
\item If you didn't have an explicit test set, how would you choose $k$? \\
\ans{
   Using cross-validation
}
}}


\section{Random Forests}

\subsection{Implementation}
\rubric{code:4,reasoning:3}

The file \emph{vowels.pkl} contains a supervised learning dataset where we are trying to predict which of the 11 ``steady-state'' English vowels that a speaker is trying to pronounce.

You are provided with a \texttt{RandomStump} class that differs from
\texttt{DecisionStumpInfoGain} in that
it only considers $\lfloor \sqrt{d} \rfloor$ randomly-chosen features.\footnote{The notation $\lfloor x\rfloor$ means the ``floor'' of $x$, or ``$x$ rounded down''. You can compute this with \texttt{np.floor(x)} or \texttt{math.floor(x)}.}
You are also provided with a \texttt{RandomTree} class that is exactly the same as
\texttt{DecisionTree} except that it uses \texttt{RandomStump} instead of
\texttt{DecisionStump} and it takes a bootstrap sample of the data before fitting.
In other words, \texttt{RandomTree} is the entity we discussed in class, which
makes up a random forest.

If you run \texttt{python main.py -q 4} it will fit a deep \texttt{DecisionTree}
using the information gain splitting criterion. You will notice that the model overfits badly.




\blu{
\enum{
\item Why doesn't the random tree model have a training error of 0? \\
\ans{
   Because the RandomStump doesn't 'considers all $d$ features it just considers 
   $\lfloor \sqrt{d} \rfloor$ randomly-chosen features. Therefore it is pretty odd that the RandomTree 
   overfits.
}
\item Create a class \texttt{RandomForest} in a file called \texttt{random\string_forest.py} that takes in hyperparameters \texttt{num\string_trees} and \texttt{max\string_depth} and
fits \texttt{num\string_trees} random trees each with maximum depth \texttt{max\string_depth}. For prediction, have all trees predict and then take the mode. \\
\ans{
   See the following code for the implementation of \texttt{RandomForest}.\\
   \begin{center}
      \lstinputlisting[firstline=4,lastline=29]{../code/random_forest.py}
  \end{center}
}
\item Using 50 trees, and a max depth of $\infty$, report the training and testing error. Compare this to what we got with a single \texttt{DecisionTree} and with a single \texttt{RandomTree}. Are the results what you expected? Discuss. \\
\ans{
   \scriptsize
   \setlength{\tabcolsep}{2pt}
   \begin{center}
   \begin{tabular}{SSSS} \toprule
      {} & {DecisionTree} & {RandomTree} & {RandomForest} \\ \midrule
      {Training error} & 0.000 & 0.133 &  0.000 \\ 
      {Test error} & 0.367 & 0.451 & 0.193 \\ \bottomrule
   \end{tabular}
   \end{center}
   \normalsize
   By seeing that the \texttt{RandomTree} doesn't have a training error of $0$ I expected the 
   \texttt{RandomForest} to don't have a training error of $0$, but it turned out that
   our \texttt{RandomForest} have a training error of 0. \\ \\ 
   This may be because of the hyperparameter \texttt{num\string_trees}. The different \texttt{RandomTree}s 
   generated may complement with each other and make the whole classifier considers all the features of 
   the training data so the model will eventually overfit.
}
\item Compare your implementation with scikit-learn's \texttt{RandomForestClassifier} for both speed and accuracy, and briefly discuss. You can use all default hyperparameters if you wish, or you can try changing them. \\ \\ \\
\ans{
   After running a few times our \texttt{RandomForest} and the \texttt{RandomForestClassifier}
   from scikit-learn with the same number of trees in the forests ($50$). We noticed that the 
   scikit-learn classifier does the work much faster than the \texttt{RandomForest} we implemented. 
   And regarding to the accuracy, we obtained the following data. \\ 
   \scriptsize
   \setlength{\tabcolsep}{10pt}
   \begin{center}
   \begin{tabular}{SSS} \toprule
      {No. run} & {RandomForest test error} & {RandomForestClassifier test error} \\ \midrule
      {1} & 0.186 & 0.178 \\ 
      {2} & 0.216 & 0.125 \\ 
      {3} & 0.208 & 0.152 \\ 
      {4} & 0.178 & 0.136 \\ 
      {5} & 0.152 & 0.186 \\ 
      {5} & 0.208 & 0.163 \\ \bottomrule
   \end{tabular}
   \end{center}
   \normalsize
   The accuracy difference between both classifier is not remarkably big, it is around $0.09$ and $0.008$. In 
   most of the cases the \texttt{RandomForestClassifier} is the one with the best accuracy.
}
}
}

\section{Clustering}

If you run \verb|python main.py -q 5|, it will load a dataset with two features
and a very obvious clustering structure. It will then apply the $k$-means algorithm
with a random initialization. The result of applying the
algorithm will thus depend on the randomization, but a typical run might look like this:
\centerfig{.5}{../figs/kmeans_basic.png}
(Note that the colours are arbitrary -- this is the label switching issue.)
But the `correct' clustering (that was used to make the data) is this:
\centerfig{.5}{../figs/kmeans_good.png}


\subsection{Selecting among $k$-means Initializations}
\rubric{reasoning:5}

If you run the demo several times, it will find different clusterings. To select among clusterings for a \emph{fixed} value of $k$, one strategy is to minimize the sum of squared distances between examples $x_i$ and their means $w_{y_i}$,
\[
f(w_1,w_2,\dots,w_k,y_1,y_2,\dots,y_n) = \sum_{i=1}^n \norm{x_i - w_{y_i}}_2^2 = \sum_{i=1}^n \sum_{j=1}^d (x_{ij} - w_{y_ij})^2.
\]
 where $y_i$ is the index of the closest mean to $x_i$. This is a natural criterion because the steps of $k$-means alternately optimize this objective function in terms of the $w_c$ and the $y_i$ values.

 \blu{\enum{
 \item In the \texttt{kmeans.py} file, add a new function called \texttt{error} that takes the same input as the \texttt{predict} function but that returns the value of this above objective function. \\
   \ans{
      See the following code for the implementation of \texttt{$K$-Means error()}.\\
      \begin{center}
         \lstinputlisting[firstline=46,lastline=54]{../code/kmeans.py}
     \end{center}
   }
 \item What trend do you observe if you print the value of this error after each iteration of the $k$-means algorithm? \\ 
   \ans{
      The error decreases after each iteration, as well as the number of changes in the cluster assignment.
   }
 \item Using the code from question 5 in \texttt{main.py} (modify if needed), output the clustering obtained by running $k$-means 50 times (with $k=4$) and taking the one with the lowest error. Submit your plot. \\
   \ans{
      \centerfig{1}{../figs/q_5_1_kmeans.pdf}
   }
 \item Looking at the hyperparameters of scikit-learn's \texttt{KMeans}, explain the first four (\texttt{n\_clusters}, \texttt{init}, \texttt{n\_init}, \texttt{max\_iter}) very briefly.
 }} 
 \ans{
    \begin{enumerate}
      \item n\_clusters\: Number of clusters (classes) to group the data
      \item init\: How the means are going to be initialized
      \item n\_init\: Number of time KMeans will be running with different seeds
      \item max\_iter\: Maximum number of iterations of KMeans, the maximum time that the means will be updated.
    \end{enumerate}
 }
 \subsection{Selecting $k$ in $k$-means}
\rubric{reasoning:5}

 We now turn to the task of choosing the number of clusters $k$.

 \blu{\enum{
% JATJ: Check
 \item Explain why we should not choose $k$ by taking the value that minimizes the \texttt{error} function. \\
    \ans{
      Because choosing the $k$ that has the lowest \texttt{error} doesn't mean that is the "correct" number of classes. 
      This approach may do the KMeans to group each training point into its own cluster, becoming super 
      specific when predicting labels.
    }
% JATJ: Check
 \item Explain why even evaluating the \texttt{error} function on test data still wouldn't be a suitable approach to choosing $k$. \\
    \ans{
      Even if we use test data to minimize the error and get $k$. We will just overfit the KMeans to have 
      the nearest means to the testing data.
    }
 \item Hand in a plot of the minimum error found across 50 random initializations, as a function of $k$, taking $k$ from $1$ to $10$. \\
    \ans{
      \centerfig{1}{../figs/q5_2_Kmeans_errors.pdf}
    }
 \item The \emph{elbow method} for choosing $k$ consists of looking at the above plot and visually trying to choose the $k$ that makes the sharpest ``elbow" (the biggest change in slope). What values of $k$ might be reasonable according to this method? Note: there is not a single correct answer here; it is somewhat open to interpretation and there is a range of reasonable answers. \\
    \ans{
      The values of $k$ that make the sharpest "elbow" on the error plot is are $k=3$ or $k=4$.
    }
 }}

\subsection{Density-Based Clustering}
\rubric{reasoning:2}

If you run \texttt{python main.py -q 5.3},
it will apply the basic density-based clustering algorithm to the dataset from the previous part, but with some outliers added.
The final output should look somewhat like this:\\
\fig{.49}{../figs/density}\fig{.49}{../figs/density2}\\
(The right plot is zoomed in to show the non-outlier part of the data.)
Even though we know that each object was generated from one of four clusters (and we have 4 outliers),
 the algorithm finds 6 clusters and does not assign some of the original non-outlier
  objects to any cluster. However, the clusters will change if we change the parameters
  of the algorithm. Find and report values for the two
  parameters, \texttt{eps} (which we called the ``radius'' in class) and \texttt{minPts},
   such that the density-based clustering method finds:
\blu{\enum{
\item The 4 ``true" clusters. \\
\ans{
   \texttt{eps} $= 2$, \texttt{minPts} $= 3$
}
\item 3 clusters (merging the top two, which also seems like a reasonable interpretaition). \\
\ans{
   \texttt{eps} $= 4$, \texttt{minPts} $= 3$
}
\item 2 clusters. \\
\ans{
   \texttt{eps} $= 13$, \texttt{minPts} $= 3$
}
\item 1 cluster (consisting of the non-outlier points). \\
\ans{
   \texttt{eps} $= 16$, \texttt{minPts} $= 3$
}
}
}



\section{Very-Short Answer Questions}
\rubric{reasoning:13}

\blu{Write a short one or two sentence answer to each of the questions below}. Make sure your answer is clear and concise.

\enum{
\item What is an advantage of using a boxplot to visualize data rather than just computing its mean and variance? \\
\ans{
   With a boxplot you can easily identify outliers and see how the data is spread.
}
\item What is a reason that the the data may not be IID in the email spam filtering example from lecture? \\
\ans{
   The bag of words of that email spam filtering example had the words \{Hi,CPSC,340,Vicodin,Offer\}. 
   The words CPSC and 340 may not be independent.
}
\item What is the difference between a validation set and a test set? \\
\ans{
   The validation set can be used during the training phase, but the test set can't be used at all 
   during the training phase
}
\item Why can't we (typically) use the training error to select a hyper-parameter? \\
\ans{
   Because the classifier will tend to overfit. Therefore if we choose a hyper-parameter using the 
   training error, our hyper-parameter will help us to overfit quicker and not to be more accurate on 
   the test data.
}
\item What is the effect of $n$ on the optimization bias (assuming we use a parametric model). \\
\ans{
   As $n$ increases, the optimization bias shrinks.
}
\item What is an advantage and a disadvantage of using a large $k$ value in $k$-fold cross-validation. \\
\ans{
   The advantage is that the cross-validation gets more accurate by using large $k$, but it gets 
   much more expensive to compute.
}
\item Why can we ignore $p(x_i)$ when we use naive Bayes? \\
\ans{
   Because $p(x_i)$ does not depends on the class that you are classifying, so $p(x_i)$ becomes constant 
   therefore we can ignore that probability.
}
\item For each of the three values below in a naive Bayes model, say whether it's a parameter or a hyper-parameter: \\
\begin{enumerate}
\item Our estimate of $p(y_i)$ for some $y_i$. \ans{parameter}
\item Our estimate of $p(x_{ij} \cond y_i)$ for some $x_{ij}$ and $y_i$. \ans{parameter}
\item The value $\beta$ in Laplace smoothing. \ans{hyperparameter}
\end{enumerate}
\item What is the effect of $k$ in KNN on the two parts (training error and approximation error) of the fundamental trade-off. Hint: think about the extreme values. \\
%JATJ: check
\ans{
   As k grows the training error gets lower and the approximation error increases.
}
\item Suppose we want to classify whether segments of raw audio represent words or not. What is an easy way to make our classifier invariant to small translations of the raw audio? \\
\ans{
   By adding transformed data to the training set
}
\item Both supervised learning and clustering models take in an input $x_i$ and produce a label $y_i$. What is the key difference? \\
\ans{
   In supervised learning the classes are known by the model. But with clustering models, the labels are unknown.
}
\item Suppose you chose $k$ in $k$-means clustering (using the squared distances to examples) from a validation set instead of a training set. Would this work better than using the training set (which just chooses the largest value of $k$)? \\
\ans{
   It will work better on the validation set but not necessarily on the training set.
}
\item In $k$-means clustering the clusters are guaranteed to be convex regions. Are the areas that are given the same label by KNN also convex? \\
%JATJ: check
\ans{
   No, the areas given by KNN not necessarily be convex. Because the data points are matched by distance,
   and in kmeans they are matched if they are inside an circular area with epsilon value. So the 
   data points in a kluster with KNN can't always create a convex area.
}
}
\end{document}
