\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage[procnames]{listings}

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}
\definecolor{ans}{rgb}{0,.5,0}%{0.545,0.27,0.074}
\def\ans#1{{\color{ans}#1}}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}

% Configuration
\usepackage[font={color=ans,bf},figurename=Fig.,labelfont={it}]{caption}
\lstset{
    language=Python, 
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{keywords},
    commentstyle=\color{comments},
    stringstyle=\color{red},
    showstringspaces=false,
    identifierstyle=\color{green},
    procnamekeys={def,class}
}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\newcommand{\matCode}[1]{\lstinputlisting[language=Matlab]{a2f/#1.m}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

\begin{document}

\title{CPSC 340 Assignment 3 (due Friday, Feb 6 at 11:55pm)}
\date{}
\maketitle

\vspace{-7em}

\section*{Instructions}
\rubric{mechanics:5}

\textbf{IMPORTANT!!! Before proceeding, please carefully read the general homework instructions at} \url{https://www.cs.ubc.ca/~fwood/CS340/homework/}. The above 5 points are for following the submission instructions. You can ignore the words ``mechanics'', ``reasoning'', etc.

\vspace{1em}
We use \blu{blue} to highlight the deliverables that you must answer/do/submit with the assignment.


\section{Finding Similar Items}

For this question we'll use the Amazon product data set\footnote{The author of the data set has asked for the following citations: (1) Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. R. He, J. McAuley. WWW, 2016, and (2) Image-based recommendations on styles and substitutes. J. McAuley, C. Targett, J. Shi, A. van den Hengel. SIGIR, 2015.} from \url{http://jmcauley.ucsd.edu/data/amazon/}. We will focus on the ``Patio, Lawn, and Garden'' section. You should start by downloading the ratings at \\
\url{https://stanford.io/2Q7QTvu} and place the file in your \texttt{data} directory with the original filename. Once you do that, running \texttt{python main.py -q 1} should do the following steps:

\begin{itemize}
\item Load the raw ratings data set into a Pandas dataframe.
\item Construct the user-product matrix as a sparse matrix (to be precise, a \verb|scipy.sparse.csr_matrix|).
\item Create bi-directional mappings from the user ID (e.g. ``A2VNYWOPJ13AFP'') to the integer index into the rows of \texttt{X}.
\item Create bi-directional mappings from the item ID (e.g. ``0981850006'') to the integer index into the columns of \texttt{X}.
\end{itemize}

\subsection{Exploratory data analysis}

\subsubsection{Most popular item}
\rubric{code:1}

Find the item with the most total stars. \blu{Submit the product name and the number of stars}.

Note: once you find the ID of the item, you can look up the name by going to the url \\ \verb|https://www.amazon.com/dp/ITEM_ID|, where \verb|ITEM_ID| is the ID of the item.
For example, the URL for item ID ``B00CFM0P7Y'' is \url{https://www.amazon.com/dp/B00CFM0P7Y}. 

\ans{
    ID: B000HCLLMM\\
    Name: Classic Accessories 73942 Veranda Grill Cover, X-Large Pebble \\
    \# Stars: 14454.0 \\
    Link: \url{https://www.amazon.com/dp/B000HCLLMM} \\
}
\centerfig{1}{../figs/B000HCLLMM}

\subsubsection{User with most reviews}
\rubric{code:1}

\blu{Find the user who has rated the most items, and the number of items they rated.} \\Â \\
\ans{
    ID: A100WO06OQR8BQ\\
    \# Items rated: 161 \\
}

\subsubsection{Histograms}
\rubric{code:2}

\blu{Make the following histograms:}
\begin{enumerate}
\item The number of ratings per user
\item The number of ratings per item
\item The ratings themselves
\end{enumerate}

Note: for the first two, use \verb|plt.yscale('log', nonposy='clip')| to put the histograms on a log-scale. Also, you can use \verb|X.getnnz| to get the total number of nonzero elements along a specific axis.

\centerfig{0.8}{../figs/ratings_per_user.pdf}
\centerfig{0.8}{../figs/ratings_per_item.pdf}
\centerfig{0.8}{../figs/ratings.pdf}

\subsection{Finding similar items with nearest neighbours}
\rubric{code:6}

We'll use scikit-learn's \texttt{neighbors.NearestNeighbors} object to find the items most similar to the example item above, namely the Brass Grill Brush 18 Inch Heavy Duty and Extra Strong, Solid Oak Handle, at URL \url{https://www.amazon.com/dp/B00CFM0P7Y}.

\blu{Find the 5 most similar items to the Grill Brush using the following metrics:}

\begin{enumerate}
\item Euclidean distance (the \texttt{NearestNeighbors} default)
\item Normalized Euclidean distance (you'll need to do the normalization)
\item Cosine similarity (by setting \texttt{metric='cosine'})
\end{enumerate}

Some notes/hints...

\begin{itemize}
\item If you run \texttt{python main.py -q 1.2}, it will grab the row of \texttt{X} associated with the grill brush. The mappers take care of going back and forther between the IDs (like ``B00CFM0P7Y'') and the indices of the sparse array ($0,1,2,\ldots$).
\item Keep in mind that scikit-learn's \texttt{NearestNeighbors} is for taking neighbors across rows, but here we're working across columns.
\item Keep in mind that scikit-learn's \texttt{NearestNeighbors} will include the query item itself as one of the nearest neighbours if the query item is in the ``training set''.
\item Normalizing the columns of a matrix would usually be reasonable to implement, but because $X$ is stored as a sparse matrix it's a bit more of a mess. Therefore, use \texttt{sklearn.preprocessing.normalize} to help you with the normalization in part 2.
\end{itemize}

\blu{Did normalized Euclidean distance and cosine similarity yields the same similar items, as expected?}

\begin{enumerate}\color{ans}
\item Euclidean distance
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SSS} \toprule
        {ID} & {Euclidean Distance} & \\ \midrule
        {B00IJB4MLA} & 74.242 \\ 
        {B00IJ8951K} & 75.146 \\ 
        {B00HVXLFZS} & 76.517 \\ 
        {B00EXDNA3O} & 76.517 \\ 
        {B00743MZCM} & 76.517 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
\item Normalized Euclidean distance
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SSS} \toprule
        {ID} & {Normalized Euclidean Distance} & \\ \midrule
        {B00IJB4MLA} & 76.385 \\ 
        {B00IJB5MCS} & 76.408 \\ 
        {B00IJ8951K} & 76.436 \\ 
        {B00EF4206E} & 76.476 \\ 
        {B00EF0I5Q2} & 76.568 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
\item Cosine similarity
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SSS} \toprule
        {ID} & {Cosine Similarity} & \\ \midrule
        {B00IJB4MLA} & 0.698 \\ 
        {B00IJB5MCS} & 0.721 \\ 
        {B00IJ8951K} & 0.749 \\ 
        {B00EF4206E} & 0.789 \\ 
        {B00EF0I5Q2} & 0.881 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
\end{enumerate}
\ans{
    As we can see in the tables above the cosine similarity yields the same similar items of the 
    Normalized Euclidean Distance. There are just two products in common of the three methods, the 
    most similar item in all methods B00IJB4MLA, and the 2nd (Euclidean Distance) or 3rd 
    (Normalized, Cosine) item  B00IJ8951K
}
\subsection{Total popularity}
\rubric{reasoning:2}

\blu{For both Euclidean distance and cosine similarity, find the number of reviews for each of the 5 recommended items and report it. Do the results make sense given what we discussed in class about Euclidean distance vs. cosine similarity and popular items?}

Note: in \texttt{main.py} you are welcome to combine this code with your code from the previous part, so that you don't have to copy/paste all that code in another section of \texttt{main.py}.
\begin{enumerate}\color{ans}
\item Euclidean distance
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SS} \toprule
        {ID} & {\# Reviews} \\ \midrule
        {B00IJB4MLA} & 45 \\ 
        {B00IJ8951K} & 1 \\ 
        {B00HVXLFZS} & 6 \\ 
        {B00EXDNA3O} & 22 \\ 
        {B00743MZCM} & 1 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
\item Cosine similarity
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SS} \toprule
        {ID} & {\# Reviews} \\ \midrule
        {B00IJB4MLA} & 45 \\ 
        {B00IJB5MCS} & 55 \\ 
        {B00IJ8951K} & 1 \\ 
        {B00EF4206E} & 1 \\ 
        {B00EF0I5Q2} & 2 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
\end{enumerate}

\section{Matrix Notation and Minimizing Quadratics}


\subsection{Converting to Matrix/Vector/Norm Notation}
\rubric{reasoning:3}

Using our standard supervised learning notation ($X$, $y$, $w$)
express the following functions in terms of vectors, matrices, and norms (there should be no summations or maximums).
\blu{\enum{
\item $\max_{i \in \{1,2,\dots,n\}}  |w^Tx_i - y_i|$. \\
\ans{
    \[
        ||Xw - y||_{\infty}
    \]
}
\item $\sum_{i=1}^n v_i(w^Tx_i  - y_i)^2 + \frac{\lambda}{2}\sum_{j=1}^d w_j^2$.
\ans{
    \[
        \text{let } V  \text{ be a } n \times n \text{ diagonal matrix with } v_i \text{ along the diagonal}
    \]
    \[
        || V (Xw  - y)^2 ||_1 + \frac{\lambda}{2} || w ||_2^2
    \]
}
\item $\left(\sum_{i=1}^n |w^Tx_i - y_i|\right)^2 +  \half\sum_{j=1}^{d} \lambda_j|w_j|$.
\ans{
    \[
        \text{let } \Lambda  \text{ be a } d \times d \text{ diagonal matrix with } \lambda_i \text{ along the diagonal}
    \]
    \[
        ||Xw - y||_{1}^2 + \half ||\Lambda w||_1
    \]
}
}}
Note that in part 2 we give a \emph{weight} $v_i$ to each training example \red{and the value $\lambda$ is a non-negative scalar}, whereas in part 3 we are regularizing the parameters with different weights $\lambda_j$.
You can use $V$ to denote a diagonal matrix that has the values $v_i$ along the diagonal, and $\Lambda$ as a diagonal matrix that has the $\lambda_j$ values along the diagonal. You can assume that all the $v_i$ and $\lambda_i$ values are non-negative. 

\subsection{Minimizing Quadratic Functions as Linear Systems}
\rubric{reasoning:3}

Write finding a minimizer $w$ of the functions below as a system of linear equations (using vector/matrix notation and simplifying as much as possible). Note that all the functions below are convex  so finding a $w$ with $\nabla f(w) = 0$ is sufficient to minimize the functions (but show your work in getting to this point).

\blu{\enum{
\item $f(w) = \half \norm{w-v}^2$ (projection of $v$ onto real space).
\ans{
    \[
        f(w) = \half (w^Tw - 2w^Tv + v^Tv)
    \]
    \[
        f(w) = \half w^Tw - w^Tv + \half v^Tv
    \]
    \[
        \nabla f(w) = w - v + 0
    \]
    \[
        0 =  w - v
    \]
    \[
        w = v
    \]
}
\item $f(w)= \half \norm{Xw - y}^2 + \half w^T\Lambda w$ (least squares with weighted regularization).
\ans{
    \[
        f(w) = \half (w^TX^TXw - 2w^TX^Ty + y^Ty) + \half w^T \Lambda w
    \]
    let $A = X^TX$ \\
    let $B = X^Ty$ \\
    let $C = y^Ty$ \\
    \[
        f(w) = \half w^TAw - w^TB + C) + \half w^T \Lambda w
    \]
    \[
        \nabla f(w) = Aw - B + 0 + \Lambda w
    \]
    \[
        0 = Aw + \Lambda w - B
    \]
    \[
        (A + \Lambda )w = B
    \]
    \[
        (X^TX + \Lambda)w = X^Ty
    \]
}
\item $f(w) = \half \sum_{i=1}^n v_i (w^Tx_i - y_i)^2 + \frac{\lambda}{2}\norm{w-w^0}^2$ (weighted least squares shrunk towards non-zero $w^0$).
\ans{
    \[
        f(w) = \half || (Xw - y)^T V (Xw - y) ||_1 + \frac{\lambda}{2}\norm{w-w^0}^2
    \]
    \[
        f(w) = \half || (V^TXw - V^Ty)^T (Xw - y) ||_1 + \frac{\lambda}{2}\norm{w-w^0}^2
    \]
    \[
        f(w) = \half || w^T X^T V^T X w - 2w^T X^T V^T y + y^T V^T y || + \frac{\lambda}{2}(w^T w - 2w^T w^0 + w^{0T} w^0)
    \]
    let $A = X^T V^T X$\\
    let $B = X^T V^T y$\\
    let $C = y^T V^T y $\\
    \[
        f(w) = \half || w^T A w - 2w^T B + C || + \frac{\lambda}{2}||w^Tw - 2w^Tw^0 + w^{0T}w^0||
    \]
    \[
        f(w) = || \half w^T A w - w^T B + \half C || + ||\frac{\lambda}{2} w^Tw - \lambda w^Tw^0 + \frac{\lambda}{2} w^{0T}w^0||
    \]
    \[
        \nabla f(w) = Aw - B + 0 + \lambda w - \lambda w^0 + 0
    \]
    \[
        0 = Aw + \lambda w - B - \lambda w^0 
    \]
    \[
        Aw + \lambda w = B - \lambda w^0 
    \]
    \[ 
        (A + \lambda I) w = B + \lambda w^0
    \]
    \[
        (X^T V^T X + \lambda I) w = X^T V^T y + \lambda w^0
    \]
}
}}
Above we assume that $v$ and $w^0$ are $d$ by $1$ vectors (in part 3 $v$ is a vector of length $n$ by 1), and $\Lambda$ is a $d$ by $d$ diagonal matrix (with positive entries along the diagonal). You can use $V$ as a diagonal matrix containing the $v_i$ values along the diagonal.

Hint: Once you convert to vector/matrix notation, you can use the results from class to quickly compute these quantities term-wise.
As a sanity check for your derivation, make sure that your results have the right dimensions. \red{As a sanity check, make that the dimensions match for all quantities/operations: in order to make the dimensions match for some parts you may need to introduce an identity matrix. For example, $X^TXw + \lambda w$ can be re-written as $(X^TX + \lambda I)w$.}



\section{Robust Regression and Gradient Descent}

If you run \verb|python main.py -q 3|, it will load a one-dimensional regression
dataset that has a non-trivial number of `outlier' data points.
These points do not fit the general trend of the rest of the data,
and pull the least squares model away from the main downward trend that most data points exhibit:
\centerfig{.7}{../figs/least_squares_outliers.pdf}

Note: we are fitting the regression without an intercept here, just for simplicity of the homework question.
In reality one would rarely do this. But here it's OK because the ``true'' line
passes through the origin (by design). In Q\ref{biasvar} we'll address this explicitly.

\subsection{Weighted Least Squares in One Dimension}
\rubric{code:3}

One of the most common variations on least squares is \emph{weighted} least squares. In this formulation, we have a weight $v_i$ for every training example. To fit the model, we minimize the weighted squared error,
\[
f(w) =  \frac{1}{2}\sum_{i=1}^n v_i(w^Tx_i - y_i)^2.
\]
In this formulation, the model focuses on making the error small for examples $i$ where $v_i$ is high. Similarly, if $v_i$ is low then the model allows a larger error. Note: these weights $v_i$ (one per training example) are completely different from the model parameters $w_j$ (one per feature), which, confusingly, we sometimes also call "weights".

Complete the model class, \texttt{WeightedLeastSquares}, that implements this model
(note that Q2.2.3 asks you to show how a few similar formulation can be solved as a linear system).
Apply this model to the data containing outliers, setting $v = 1$ for the first
$400$ data points and $v = 0.1$ for the last $100$ data points (which are the outliers).
\blu{Hand in your code and the updated plot}.

\begin{center}
    \lstinputlisting[firstline=14,lastline=20]{../code/linear_model.py}
\end{center}

\centerfig{1}{../figs/weighted_least_squares_outliers.pdf}

\subsection{Smooth Approximation to the L1-Norm}
\rubric{reasoning:3}

Unfortunately, we typically do not know the identities of the outliers. In situations where we suspect that there are outliers, but we do not know which examples are outliers, it makes sense to use a loss function that is more robust to outliers. In class, we discussed using the sum of absolute values objective,
\[
f(w) = \sum_{i=1}^n |w^Tx_i - y_i|.
\]
This is less sensitive to outliers than least squares, but it is non-differentiable and harder to optimize. Nevertheless, there are various smooth approximations to the absolute value function that are easy to optimize. One possible approximation is to use the log-sum-exp approximation of the max function\footnote{Other possibilities are the Huber loss, or $|r|\approx \sqrt{r^2+\epsilon}$ for some small $\epsilon$.}:
\[
|r| = \max\{r, -r\} \approx \log(\exp(r) + \exp(-r)).
\]
Using this approximation, we obtain an objective of the form
\[
f(w) {=} \sum_{i=1}^n  \log\left(\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)\right).
\]
which is smooth but less sensitive to outliers than the squared error. \blu{Derive
 the gradient $\nabla f$ of this function with respect to $w$. You should show your work but you do \underline{not} have to express the final result in matrix notation.}
\ans{
    \[
        \nabla f(w) = \frac{x_i\exp(w^Tx_i - y_i) - x_i\exp(y_i - w^Tx_i)}{\exp(w^Tx_i - y_i) + \exp(y_i - w^Tx_i)}
    \]
}

\subsection{Robust Regression}
\rubric{code:3}

The class \texttt{LinearModelGradient} is the same as \texttt{LeastSquares}, except that it fits the least squares model using a gradient descent method. If you run \verb|python main.py -q 3.3| you'll see it produces the same fit as we obtained using the normal equations.

The typical input to a gradient method is a function that, given $w$, returns $f(w)$ and $\nabla f(w)$. See \texttt{funObj} in \texttt{LinearModelGradient} for an example. Note that the \texttt{fit} function of \texttt{LinearModelGradient} also has a numerical check that the gradient code is approximately correct, since implementing gradients is often error-prone.\footnote{Sometimes the numerical gradient checker itself can be wrong. See CPSC 303 for a lot more on numerical differentiation.}

An advantage of gradient-based strategies is that they are able to solve
problems that do not have closed-form solutions, such as the formulation from the
previous section. The class \texttt{LinearModelGradient} has most of the implementation
of a gradient-based strategy for fitting the robust regression model under the log-sum-exp approximation.
The only part missing is the function and gradient calculation inside the \texttt{funObj} code.
\blu{Modify \texttt{funObj} to implement the objective function and gradient based on the smooth
approximation to the absolute value function (from the previous section). Hand in your code, as well
as the plot obtained using this robust regression approach.}

\begin{center}
    \lstinputlisting[firstline=40,lastline=48]{../code/linear_model.py}
\end{center}

\centerfig{1}{../figs/least_squares_robust.pdf}



\section{Linear Regression and Nonlinear Bases}

In class we discussed fitting a linear regression model by minimizing the squared error.
In this question, you will start with a data set where least squares performs poorly.
You will then explore how adding a bias variable and using nonlinear (polynomial) bases can drastically improve the performance.
You will also explore how the complexity of a basis affects both the training error and the test error.

\subsection{Adding a Bias Variable}
\label{biasvar}
\rubric{code:3}

If you run  \verb|python main.py -q 4|, it will:
\enum{
\item Load a one-dimensional regression dataset.
\item Fit a least-squares linear regression model.
\item Report the training error.
\item Report the test error (on a dataset not used for training).
\item Draw a figure showing the training data and what the linear model looks like.
}
Unfortunately, this is an awful model of the data. The average squared training error on the data set is over 28000
(as is the test error), and the figure produced by the demo confirms that the predictions are usually nowhere near
 the training data:
\centerfig{.5}{../figs/least_squares_no_bias.pdf}
The $y$-intercept of this data is clearly not zero (it looks like it's closer to $200$),
so we should expect to improve performance by adding a \emph{bias} (a.k.a. intercept) variable, so that our model is
\[
y_i = w^Tx_i + w_0.
\]
instead of
\[
y_i = w^Tx_i.
\]
\blu{In file \texttt{linear\string_model.py}, complete the class, \texttt{LeastSquaresBias},
that has the same input/model/predict format as the \texttt{LeastSquares} class,
but that adds a \emph{bias} variable (also called an intercept) $w_0$ (also called $\beta$ in lecture). Hand in your new class, the updated plot,
and the updated training/test error.}

Hint: recall that adding a bias $w_0$ is equivalent to adding a column of ones to the matrix $X$. Don't forget that you need to do the same transformation in the \texttt{predict} function.

\begin{center}
    \lstinputlisting[firstline=49,lastline=62]{../code/linear_model.py}
\end{center}

\centerfig{1}{../figs/least_squares_bias.pdf}
\ans{
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SSS} \toprule
        {Model} & {Train error} & {Test error} \\ \midrule
        {LeastSquares} & 28122.8 & 28299.0 \\ 
        {LeastSquaresBias} & 3551.3 & 3393.9 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
}

\subsection{Polynomial Basis}
\rubric{code:4}

Adding a bias variable improves the prediction substantially, but the model is still problematic because the target seems to be a \emph{non-linear} function of the input.
Complete \texttt{LeastSquarePoly} class, that takes a data vector $x$ (i.e., assuming we only have one feature) and the polynomial order $p$. The function should perform a least squares fit based on a matrix $Z$ where each of its rows contains the values $(x_{i})^j$ for $j=0$ up to $p$. E.g., \texttt{LeastSquaresPoly.fit(x,y)}  with $p = 3$ should form the matrix
\[
Z =
\left[\begin{array}{cccc}
1 & x_1 & (x_1)^2 & (x_1)^3\\
1 & x_2 & (x_2)^2 & (x_2)^3\\
\vdots\\
1 & x_n & (x_n)^2 & (x_N)^3\\
\end{array}
\right],
\]
and fit a least squares model based on it.
\blu{Hand in the new class, and report the training and test error for $p = 0$ through $p= 10$. Explain the effect of $p$ on the training error and on the test error.}

Note: you should write the code yourself; don't use a library like sklearn's \texttt{PolynomialFeatures}.

\begin{center}
    \lstinputlisting[firstline=72,lastline=93]{../code/linear_model.py}
\end{center}

\centerfig{1}{../figs/least_squares_poly.pdf}
\ans{
    \scriptsize
    \setlength{\tabcolsep}{10pt}
    \begin{center}
    \begin{tabular}{SSS} \toprule
        {$p$} & {Train error} & {Test error} \\ \midrule
        {$p=0$} & 15480.5 & 14390.763 \\ 
        {$p=1$} & 3551.34 & 3393.869 \\ 
        {$p=2$} & 2167.99 & 2480.725 \\ 
        {$p=3$} & 252.046 & 242.804 \\ 
        {$p=4$} & 251.461 & 242.126 \\ 
        {$p=5$} & 251.143 & 239.544 \\ 
        {$p=6$} & 248.582 & 246.005 \\ 
        {$p=7$} & 247.011 & 242.887 \\ 
        {$p=8$} & 241.306 & 245.966 \\ 
        {$p=9$} & 235.761 & 259.296 \\ 
        {$p=10$} & 235.074 & 256.299 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
    As the value of $p$ grows the error (training and testing) of the model decreases significantly, 
    until $p=3$ from there the training errors start decreasing slowly around $250$ and on $p=7$ 
    the training error continue decreasing whereas the testing error start to increase. The model is 
    just overfitting as $p$ grows.
}

\section{Very-Short Answer Questions}
\rubric{reasoning:7}

\begin{enumerate}
\item Suppose that a training example is global outlier, meaning it is really far from all other data points. How is the cluster assignment of this example by $k$-means? And how is it set by density-based clustering?\\
\ans{
    $k$-means will cluster it to the "nearest" cluster it founds whereas density-based clustering 
    will not assigned it to any cluster, because it may not be at the $eps$ distance of any other 
    training point.
}
\item Why do need random restarts for $k$-means but not for density-based clustering?\\
\ans{
    Because $k$-means its severely affected by the initial position of the mean points, it may find 
    different clusters if the two mean points are initialized pretty close. In the other hand 
    density-based clustering do not depend on any position, but the hyperparameters $eps$ and 
    $min_pts$. If those hyperparameters do not change, the results of the density-based clustering 
    will be the same on every run.
}
\item Can hierarchical clustering find non-convex clusters?\\
\ans{
    It depends on the metrics used to create the clusters, then the hierarchical clusters found may or 
    not be convex.
}
\item For model-based outlier detection, list an example method and problem with identifying outliers using this method.\\
\ans{
    An example method will be using the z-score $z_i = \frac{x_i - \mu}{\sigma}$, to calculate the 
    number of standard deviations away from the mean. The problem with this method is that $ \mu \and 
    \sigma$ are sensitive to outliers, so maybe an outlier would move so much the $ \mu $ that we will 
    not identify it correctly as an outlier.
}
\item For graphical-based outlier detection, list an example method and problem with identifying outliers using this method.\\
\ans{
    An example method will be using an scatterplot to visualize the data set and decide what points 
    are outliers. The problem of this method arises when the dimension of the feature grows. Because it 
    is not easy to visualize data in more than $3d$, the method may have problems with datasets with 
    high number of features.
}
\item For supervised outlier detection, list an example method and problem with identifying outliers using this method.\\
\ans{
    We can use random forests to detect outliers, the problem with this is that we need to either 
    add outliers to the training set or first identify some outliers using another detection method. 
    Ahd then create the training set with the labels of outlier and not-outlier.
}
\item If we want to do linear regression with 1 feature, explain why it would or would not make sense to use gradient descent to compute the least squares solution.\\
\ans{
    By using just the normal equations the model is not robust enough to detect outliers so it may 
    not be a great model to predict the data.
}
\item Why do we typically add a column of $1$ values to $X$ when we do linear regression? Should we do this if we're using decision trees?\\
\ans{
    To add a \emph{bias} or \emph{intercept} to the model, with this we enable the model to not 
    necessarily pass through the origin.
}
\item If a function is convex, what does that say about stationary points of the function? Does convexity imply that a stationary points exists?\\
\ans{
    If a function is convex then every stationary point will be a global minima. And if the function is 
    convex will always have stationary points.
}
\item Why do we need gradient descent for the robust regression problem, as opposed to just using the normal equations? Hint: it is NOT because of the non-differentiability. Recall that we used gradient descent even after smoothing away the non-differentiable part of the loss.\\
\ans{
    Because it is faster to use rather than solving the normal equations.
}
\item What is the problem with having too small of a learning rate in gradient descent?\\
\ans{
    It may take a lot of time to converge to the minima, because it will approximate to it at a 
    slow pace.
}
\item What is the problem with having too large of a learning rate in gradient descent?\\
\ans{
    It may oscillate in the function while approximating to the minima, which will also make longer 
    the time of convergence.
}
\item What is the purpose of the log-sum-exp function and how is this related to gradient descent?\\
\ans{
    To make an smooth approximation of the $L_\infty$ norm, because minimization of the max function is 
    much harder, we create an smooth approximation using $log()$ and $exp()$ which will be much similar 
    to the $max$ function and we can use gradient descent on it.
}
\item What type of non-linear transform might be suitable if we had a periodic function?\\
\ans{
    A trigonometric transform will help with the periodic function.
}
\end{enumerate}

\end{document}
