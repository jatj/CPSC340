\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{nicefrac}
% \usepackage{listings} % For displaying code
\usepackage{algorithm2e} % pseudo-code
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage[procnames]{listings}
\usepackage{soul}

\def\rubric#1{\gre{Rubric: \{#1\}}}{}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}
\definecolor{ans}{rgb}{0,.5,0}%{0.545,0.27,0.074}
\def\ans#1{{\color{ans}#1}}
\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}

% Configuration
\usepackage[font={color=ans,bf},figurename=Fig.,labelfont={it}]{caption}
\lstset{
    language=Python, 
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{keywords},
    commentstyle=\color{comments},
    stringstyle=\color{red},
    showstringspaces=false,
    identifierstyle=\color{green},
    procnamekeys={def,class}
}

% Math
\def\R{\mathbb{R}}
\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\alignStar}[1]{\begin{align*}#1\end{align*}}
\def\half{\frac 1 2}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}

\begin{document}


\title{CPSC 340 Assignment 5 (due Monday March 18 at 11:55pm)}
\date{}
\maketitle

\vspace{-4em}
\ans{
    Student name: José Abraham Torres Juárez \\
    Student id: 79507828 \\ \\
}


\section*{Instructions}
\rubric{mechanics:5}

\textbf{IMPORTANT!!! Before proceeding, please carefully read the general homework instructions at} \url{https://www.cs.ubc.ca/~fwood/CS340/homework/}. The above 5 points are for following the submission instructions. You can ignore the words ``mechanics'', ``reasoning'', etc.

\vspace{1em}
We use \blu{blue} to highlight the deliverables that you must answer/do/submit with the assignment.


\section{Kernel Logistic Regresion}

If you run \verb|python main.py -q 1| it will load a synthetic 2D data set, split it into train/validation sets, and then perform regular logistic regression and kernel logistic regression (both without an intercept term, for simplicity). You'll observe that the error values and plots generated look the same since the kernel being used is the linear kernel (i.e., the kernel corresponding to no change of basis).
\clearpage
\subsection{Implementing kernels}
\rubric{code:5}

 \blu{Implement the polynomial kernel and the RBF kernel for logistic regression. Report your training/validation errors and submit the plots generated for each case}. You should use the hyperparameters $p=2$ and $\sigma=0.5$ respectively, and $\lambda=0.01$ for the regularization strength.
 \ans{
    \begin{center}
        \lstinputlisting[firstline=47,lastline=57]{../code/logReg.py}
    \end{center}
    \begin{figure}[htp]
        \begin{subfigure}[b]{0.5\textwidth} \color{ans}
            \centerfig{1.0}{../figs/logRegPolynomialKernel.png}
            {
              \begin{center}
                 Polynomial Kernel
              \end{center}
            }
          \label{fig:1}
        \end{subfigure}
        %
        \begin{subfigure}[b]{0.5\textwidth} \color{ans}
            \centerfig{1.0}{../figs/logRegRBFKernel.png}
           {
               \begin{center}
                RBF Kernel
               \end{center}
           }
          \label{fig:2}
        \end{subfigure}
    \end{figure}
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{center}
    \begin{tabular}{SSS} \toprule
       {} & {Training error} & {Validation Error} \\ \midrule
       {Polynomial kernel} & 0.183 & 0.170 \\ \midrule
       {RBF kernel} & 0.150 & 0.130 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
 }
 \clearpage
\subsection{Hyperparameter search}
\rubric{code:3}

For the RBF kernel logistic regression, consider the hyperparameters values $\sigma=10^m$ for $m=-2,-1,\ldots,2$ and $\lambda=10^m$ for $m=-4,-3,\ldots,0$. \blu{In \texttt{main.py}, sweep over the possible combinations of these hyperparameter values. Report the hyperparameter values that yield the best training error and the hyperparameter values that yield the best validation error. Include the plot for each.}

Note: on the job you might choose to use a tool like scikit-learn's \texttt{GridSearchCV} to implement the grid search, but here we are asking you to implement it yourself by looping over the hyperparameter values.

\ans{
    \begin{figure}[htp]
        \begin{subfigure}[b]{0.5\textwidth} \color{ans}
            \centerfig{1.0}{../figs/logRegRBFKernel_MaxTrain.png}
            {
              \begin{center}
                 RBF Kernel with best training error
              \end{center}
            }
          \label{fig:1}
        \end{subfigure}
        %
        \begin{subfigure}[b]{0.5\textwidth} \color{ans}
            \centerfig{1.0}{../figs/logRegRBFKernel_MaxValidation.png}
           {
               \begin{center}
                RBF Kernel with best validation error
               \end{center}
           }
          \label{fig:2}
        \end{subfigure}
    \end{figure}
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{center}
    \begin{tabular}{SSSSS} \toprule
       {} & {Training error} & {Validation Error} & {$\sigma$} & {$\lambda$} \\ \midrule
       {Best training} & 0.017 & 0.210 & 0.010 & 0.0001 \\ \midrule
       {Best validation} & 0.113 & 0.110 & 0.100 & 0.0001 \\ \bottomrule
    \end{tabular}
    \end{center}
    \normalsize
}


\subsection{Reflection}
\rubric{reasoning:1}

Briefly discuss the best hyperparameters you found in the previous part, and their associated plots. Was the training error minimized by the values you expected, given the ways that $\sigma$ and $\lambda$ affect the fundamental tradeoff?

\ans{
    As we can tell by the images above, the value of $\lambda$ doesn't change, it appears to 
    be the sweet spot for this data set. On the other hand the value of $\sigma$ changes, as it grows
    the validation training error becomes lower but the approximation error also grows. \\ \\ 
    So we can conclude that as $\sigma$ grows from 0.01 to 0.1 the approximation error becomes smaller 
    which yields a better validation error.
}


\section{MAP Estimation}
\rubric{reasoning:8}

In class, we considered MAP estimation in a regression model where we assumed that:
\items{
\item The likelihood $p(y_i \mid x_i, w)$ is a normal distribution with a mean of $w^Tx_i$ and a variance of $1$.
\item The prior for each variable $j$, $p(w_j)$, is a normal distribution with a mean of zero and a variance of $\lambda^{-1}$.
}
Under these assumptions, we showed that this leads to the standard L2-regularized least squares objective function,
\[
f(w) = \frac{1}{2}\norm{Xw - y}^2 + \frac \lambda 2 \norm{w}^2,
\]
which is the negative log likelihood (NLL) under these assumptions (ignoring an irrelevant constant).
\blu{For each of the alternate assumptions below, show how the loss function would change} (simplifying as much as possible):
\enum{
\item We use a Laplace likelihood with a mean of $w^Tx_i$ and a scale of $1$, and we use a zero-mean Gaussian prior with a variance of $\sigma^2$.
\[
p(y_i \mid x_i, w) = \frac 1 2 \exp(-|w^Tx_i - y_i|), \quad p(w_j) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{w_j^2}{2\sigma^2}\right).
\]
\ans{
    \[ f(w) = -\sum_{i=1}^{n} \log \left[ \half{\exp(-| w^Tx_i - y_i |)} \right] - \log \left[ \frac{1}{2\pi\sigma} \exp(-\sum_{j=1}^d \frac{w_j^2}{2\sigma^2}) \right] \]
    \[ f(w) = -\sum_{i=1}^{n} \left[ \log(\half) + \log(\exp(-| w^Tx_i - y_i |)) \right] - \log(\frac{1}{2\pi\sigma}) - \log \left[ \exp(-\sum_{j=1}^d \frac{w_j^2}{2\sigma^2}) \right] \]
    \[ f(w) = -\sum_{i=1}^{n} \left[ constant + \log(\exp(-| w^Tx_i - y_i |)) \right] - constant + \sum_{j=1}^d \frac{w_j^2}{2\sigma^2} \]
    \[ f(w) =  constant -\sum_{i=1}^{n} \left[ -| w^Tx_i - y_i |) \right] - constant + \frac{1}{2\sigma^2} \sum_{j=1}^d w_j^2 \]
    \[ f(w) =  constant +\sum_{i=1}^{n} \left[ | w^Tx_i - y_i |) \right] + \frac{1}{2\sigma^2} || w ||_2^2 \]
    \[ f(w) =  constant + || Xw - Y ||_1 + \frac{1}{2\sigma^2} || w ||_2^2 \]
}
\item We use a Gaussian likelihood where each datapoint has its own variance $\sigma_i^2$, and where we use a zero-mean Laplace prior with a vairance of $\lambda^{-1}$.
\[
p(y_i \mid x_i,w) = \frac{1}{\sqrt{2\sigma_i^2\pi}}\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right), \quad p(w_j) = \frac{\lambda}{2}\exp(-\lambda|w_j|).
\]
You can use $\Sigma$ as a diagonal matrix that has the values $\sigma_i^2$ along the diagonal.
\ans{
    \[ f(w) = - \sum_{i=1}^n \log \left[ \frac{1}{\sqrt{2\sigma_i^2 \pi}} \exp(-\frac{(w^Tx_i-y_i)^2}{2\sigma_i^2}) \right] - \log \left[ \frac{\lambda}{2} \exp \left(-\sum_{j=1}^d \lambda | w_j| \right) \right] \]
    \[ f(w) = - \sum_{i=1}^n \left[ \log \left(\frac{1}{\sqrt{2\sigma_i^2 \pi}}\right) + \log \left( \exp(-\frac{(w^Tx_i-y_i)^2}{2\sigma_i^2}) \right) \right] - \log \left(\frac{\lambda}{2}\right) - log \left[ \exp \left(-\sum_{j=1}^d \lambda | w_j| \right) \right] \]
    \[ f(w) = - \sum_{i=1}^n \left[ constant -\frac{(w^Tx_i-y_i)^2}{2\sigma_i^2} \right] - constant + \sum_{j=1}^d \lambda | w_j|  \]
    \[ f(w) = constant - \sum_{i=1}^n \left[ -\frac{(w^Tx_i-y_i)^2}{2\sigma_i^2} \right] + \sum_{j=1}^d \lambda | w_j|  \]
    \[ f(w) = constant + || \frac{(Xw - Y)^2}{2\Sigma} ||_1 + \sum_{j=1}^d \lambda | w_j|  \]
}
\item We use a (very robust) student $t$ likelihood with a mean of $w^Tx_i$ and $\nu$ degrees of freedom, and a zero-mean Gaussian prior with a variance of $\lambda^{-1}$,
\[
p(y_i | x_i, w) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac \nu 2\right)}\left(1 + \frac{(w^Tx_i - y_i)^2}{\nu}\right)^{-\frac{\nu+1}{2}}, \quad p(w_j) = \frac{\sqrt{\lambda}}{\sqrt{2\pi}}\exp\left(-\lambda\frac{w_j^2}{2}\right).
\]
where $\Gamma$ is the ``gamma" function (which is always non-negative).
\ans{
    \[ f(w) = - \sum_{i=1}^n \left[ \log\left(\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt(\nu\pi)\Gamma(\frac{\nu}{2})}  \left(1 + \frac{(w^Tx_i - y_i)^2}{\nu}\right)^{-\frac{\nu+1}{2}} \right) \right] - \log \left[ \frac{\sqrt{\lambda}}{\sqrt{2\pi}} \exp\left(-\sum_{j=1}^d \lambda \frac{w_j^2}{2}\right)\right]  \]
    \[ f(w) = - \sum_{i=1}^n \left[ \log\left(\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt(\nu\pi)\Gamma(\frac{\nu}{2})} \right) + \log \left( \left(1 + \frac{(w^Tx_i - y_i)^2}{\nu}\right)^{-\frac{\nu+1}{2}} \right) \right] - \log \left( \frac{\sqrt{\lambda}}{\sqrt{2\pi}}\right) - \log \left[ \exp\left(-\sum_{j=1}^d \lambda \frac{w_j^2}{2}\right)\right]  \]
    \[ f(w) = - \sum_{i=1}^n \left[ constant + \log \left( \left(1 + \frac{(w^Tx_i - y_i)^2}{\nu}\right)^{-\frac{\nu+1}{2}} \right) \right] - constant + \sum_{j=1}^d \lambda \frac{w_j^2}{2} \]
    \[ f(w) = constant - \sum_{i=1}^n \left[ \log \left( \left(1 + \frac{(w^Tx_i - y_i)^2}{\nu}\right)^{-\frac{\nu+1}{2}} \right) \right] + \frac{\lambda}{2} ||w||_2^2 \]
    \[ f(w) = constant - \log \left( \left(1 + \frac{1}{\nu}|| Xw - Y ||_2^2\right)^{-\frac{\nu+1}{2}} \right) + \frac{\lambda}{2} ||w||_2^2 \]
}
\item We use a Poisson-distributed likelihood (for the case where $y_i$ represents counts), and we use a uniform prior for some constant $\kappa$,
\[
p(y_i | w^Tx_i) = \frac{\exp(y_iw^Tx_i)\exp(-\exp(w^Tx_i))}{y_i!}, \quad p(w_j) \propto \kappa.
\]
(This prior is 	``improper'' since $w\in\R^d$ but it doesn't integrate to 1 over this domain, but nevertheless the posterior will be a proper distribution.)
\ans{
    \[ f(w) = - \sum_{i=1}^n \log \left[ \frac{\exp(y_iw^Tx_i)\exp(-\exp(w^Tx_i))}{y_i!} \right] - \log(k) \]
    \[ f(w) = - \sum_{i=1}^n \left[ \log \left( \frac{1}{y_i!} \right) + \log \left( \exp(y_iw^Tx_i) \right) + \log \left( \exp(-\exp(w^Tx_i)) \right) \right] - \log(k) \]
    \[ f(w) = - \sum_{i=1}^n \log \left( \frac{1}{y_i!} \right) - \sum_{i=1}^n \log \left( \exp(y_iw^Tx_i) \right) - \sum_{i=1}^n \log \left( \exp(-\exp(w^Tx_i)) \right) - \log(k) \]
    \[ f(w) = - || \log \left( \frac{1}{y!} \right) ||_1 - \sum_{i=1}^n y_iw^Tx_i + \sum_{i=1}^n \exp(w^Tx_i) - \log(k) \]
    \[ f(w) = - || \log \left( \frac{1}{y!} \right) ||_1 - y^TXw + || \exp(Xw) ||_1 - \log(k) \]
}
}

\section{Principal Component Analysis}
\rubric{reasoning:3}


Consider the following dataset, containing 5 examples with 2 features each:
\begin{center}
\begin{tabular}{cc}
$x_1$ & $x_2$\\
\hline
-4 & 3\\
0 & 1\\
-2 & 2\\
4 & -1\\
2 & 0\\
\end{tabular}
\end{center}
Recall that with PCA we usually assume that the PCs are normalized ($\norm{w} = 1$), we need to center the data before we apply PCA, and that the direction of the first PC is the one that minimizes the orthogonal distance to all data points.
\blu{
\enum{
\item What is the first principal component? \\
\ans{
    The first principal component is $(\frac{-2}{\sqrt{5}}, \frac{1}{\sqrt{5}})$
}
\item What is the reconstruction loss (L2 norm squared) of the point (-3, 2.5)? (Show your work.)
\ans{
    Using the first principal component we transform the point $x = (-3, 2.5)$,
    \[ 
        \begin{bmatrix} 
            -3 & 2.5
        \end{bmatrix}
        \times  
        \begin{bmatrix} 
            -\frac{2}{\sqrt{5}} \\
            \frac{1}{\sqrt{5}}
        \end{bmatrix}     
        = \frac{8.5}{\sqrt{5}}
    \]
    Now we calculate the error with the L2 norm squared of $x \text{and} \hat{x}$
    \[ 
        loss = \left|\left| \frac{8.5}{\sqrt{5}} \times \begin{bmatrix} \frac{-2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} \end{bmatrix} - \begin{bmatrix} -3 \\ 2.5 \end{bmatrix} \right|\right|_2^2 = 
        \left|\left| \begin{bmatrix} \frac{-17}{5} \\ \frac{8.5}{5} \end{bmatrix} - \begin{bmatrix} -3 \\ 2.5 \end{bmatrix} \right|\right|_2^2 = 
        \left|\left| \begin{bmatrix} \frac{-2}{5} \\ \frac{-4}{5}\end{bmatrix} \right|\right|_2^2 = 
        \frac{4}{25} + \frac{16}{25} = \frac{20}{25} = \fbox{0.8}
    \]
}
\item What is the reconstruction loss (L2 norm squared) of the point (-3, 2)? (Show your work.)
\ans{
    Using the first principal component we transform the point $x = (-3, 2)$,
    \[ 
        \begin{bmatrix} 
            -3 & 2
        \end{bmatrix}
        \times  
        \begin{bmatrix} 
            -\frac{2}{\sqrt{5}} \\
            \frac{1}{\sqrt{5}}
        \end{bmatrix}     
        = \frac{8}{\sqrt{5}}
    \]
    Now we calculate the error with the L2 norm squared of $x \text{and} \hat{x}$
    \[ 
        loss = \left|\left| \frac{8}{\sqrt{5}} \times \begin{bmatrix} \frac{-2}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} \end{bmatrix} - \begin{bmatrix} -3 \\ 2.5 \end{bmatrix} \right|\right|_2^2 = 
        \left|\left| \begin{bmatrix} \frac{-17}{5} \\ \frac{8}{5} \end{bmatrix} - \begin{bmatrix} -3 \\ 2.5 \end{bmatrix} \right|\right|_2^2 = 
        \left|\left| \begin{bmatrix} \frac{-1}{5} \\ \frac{-2}{5}\end{bmatrix} \right|\right|_2^2 = 
        \frac{1}{25} + \frac{4}{25} = \frac{5}{25} = \fbox{0.2}
    \]
}
}
}
Hint: it may help (a lot) to plot the data before you start this question.



\section{PCA Generalizations}

\subsection{Robust PCA}
\rubric{code:10}

If you run \verb|python main -q 4.1| the code will load a dataset $X$ where each row contains the pixels from a single frame of a video of a highway. The demo applies PCA to this dataset and then uses this to reconstruct the original image.
It then shows the following 3 images for each frame:
\enum{
\item The original frame.
\item The reconstruction based on PCA.
\item A binary image showing locations where the reconstruction error is non-trivial.
}
Recently, latent-factor models have been proposed as a strategy for ``background subtraction'': trying to separate objects from their background. In this case, the background is the highway and the objects are the cars on the highway. In this demo, we see that PCA does an OK job of identifying the cars on the highway in that it does tend to identify the locations of cars. However, the results aren't great as it identifies quite a few irrelevant parts of the image as objects.

Robust PCA is a variation on PCA where we replace the L2-norm with the L1-norm,
\[
f(Z,W) = \sum_{i=1}^n\sum_{j=1}^d |\langle w^j, z_i\rangle - x_{ij}|,
\]
and it has recently been proposed as a more effective model for background subtraction. \blu{Complete the class \emph{pca.RobustPCA},
that uses a smooth approximation to the absolute value to implement robust PCA. Briefly comment on the results.} 

Note: in its current state, \emph{pca.RobustPCA} is just a copy of \emph{pca.AlternativePCA}, which is why the two rows of images are identical.

Hint: most of the work has been done for you in the class \emph{pca.AlternativePCA}.
This work implements an alternating minimization approach to minimizing the (L2) PCA objective (without enforcing orthogonality). This gradient-based approach to PCA can be modified to use a smooth approximation of the L1-norm. Note that the log-sum-exp approximation to the absolute value may be hard to get working due to numerical issues, and a numerically-nicer approach is to use the ``multi-quadric'' approximation:
\[
|\alpha| \approx \sqrt{\alpha^2 + \epsilon},
\]
where $\epsilon$ controls the accuracy of the approximation (a typical value of $\epsilon$ is $0.0001$).

\begin{center}
    \lstinputlisting[firstline=81,lastline=102]{../code/pca.py}
\end{center}

\ans{
    Losses of alternative pca
    \begin{center}
    \begin{tabular}{cc}
    $Interation$ & $loss$\\
    \hline
    0 &  8020.7 \\
    1 &  7146.9 \\
    2 &  6940.3 \\
    3 &  6861.5 \\
    4 &  6826.9 \\
    5 &  6804.4 \\
    6 &  6785.7 \\
    7 &  6770.5 \\
    8 &  6764.5 \\
    9 &  6764.5 \\
    \end{tabular}
    \end{center}
    Losses of robsut pca
    \begin{center}
    \begin{tabular}{cc}
    $Interation$ & $loss$\\
    \hline
    0 & 116386.2 \\
    1 & 96108.6 \\
    2 & 94137.2 \\
    3 & 93078.1 \\
    4 & 92427.2 \\
    5 & 91884.5 \\
    6 & 91449.0 \\
    7 & 91211.6 \\
    8 & 91068.3 \\
    9 & 91000.1 \\
    \end{tabular}
    \end{center}
    \centerfig{0.5}{../figs/highway_000.png}
    \centerfig{0.5}{../figs/highway_001.png}
    \centerfig{0.5}{../figs/highway_002.png}
    \centerfig{0.5}{../figs/highway_003.png}
    \centerfig{0.5}{../figs/highway_004.png}
    \centerfig{0.5}{../figs/highway_005.png}
    \centerfig{0.5}{../figs/highway_006.png}
    \centerfig{0.5}{../figs/highway_007.png}
    \centerfig{0.5}{../figs/highway_008.png}
    \centerfig{0.5}{../figs/highway_009.png}
}

\subsection{Reflection}
\rubric{reasoning:3}

\enum{
\item Briefly explain why using the L1 loss might be more suitable for this task than L2. \\
\ans{
    The L2 norm gives too much attention to data that is probably not a car in the highway. But the 
    L1 loss doesn't do this so it is more likely to give better performance at detecting changes.
}
\item How does the number of video frames and the size of each frame relate to $n$, $d$, and/or $k$? \\
\ans{
    $n$ is the number of frames, $d$ depends on the size of the image. And $k$ 
    depends on the whole dataset and how accurate do we want to represent our 
    data, if we one super accurate representation we will use more, PCs than if we want a no so 
    accurate representation.
}
\item What would the effect be of changing the threshold (see code) in terms of false positives (cars we identify that aren't really there) and false negatives (real cars that we fail to identify)? \\
\ans{
    The threshold is the color that a pixel has to be considered as an object on the highway. So if 
    we change it, it will either consider more things as an object or the highway (false positive) 
    or make harder to find objects on the highway (false negatives).
}
}

\section{Very-Short Answer Questions}
\rubric{reasoning:11}

\enum{
\item Assuming we want to use the original features (no change of basis) in a linear model, what is an advantage of the ``other'' normal equations over the original normal equations?  \\
\ans{
    If we have a pretty huge number of features and a small number of examples, so $n < d$. 
    It will be faster to compute.
}
\item In class we argued that it's possible to make a kernel version of $k$-means clustering. What would an advantage of kernels be in this context?  \\
\ans{
    That we are able to find a global optima.
}
\item In the language of loss functions and regularization, what is the difference between MLE and MAP?  \\
\ans{
    MLE is just a loss function, and MAP is a loss function with regularization.
}
\item What is the difference between a generative model and a discriminative model?  \\
\ans{
    A generative model, models how the data is created so it uses probability rules to create predictions 
    on what will be the features of some new example of data. Whereas the discriminative model fixes 
    the features and create a model that correctly represent the provided features and also allow us 
    to interpolate and extrapolate data.
}
\item With PCA, is it possible for the loss to increase if $k$ is increased? Briefly justify your answer.  \\
\ans{
    It shouldn't be possible. As $k$ grows you are just incrementing the ways in how you represent the 
    data, this yields to more accurate representations of the data which result in a lower loss.
}
\item What does ``label switching'' mean in the context of PCA?  \\
\ans{
    We can fine multiple values of $Z$ and $W$ that yield a correct $\hat{X}$
}
\item Why doesn't it make sense to do PCA with $k > d$?  \\
\ans{
    Because with $k = d$ you can totally represent the data. There is no need to transform the data 
    so it will be just a waste of resources.
}
\item In terms of the matrices associated with PCA ($X$, $W$, $Z$, $\hat{X}$), where would an ``eigenface'' be stored?  \\
\ans{
    The eigenfaces would be stored on $Z$.
}
\item What is an advantage and a disadvantage of using stochatic gradient over SVD when doing PCA?  \\
\ans{
    A disadvantage is that the solution using stochastic gradient might not be unique, whereas the 
    SVD leads to a unique solution. An advantage of stochastic gradient is that it should be faster 
    to compute which allow us to use huge data sets.
}
\item Which of the following step-size sequences lead to convergence of stochastic gradient to a stationary point?  \\
\enum{
\item $\alpha^t = 1/t^2$.
\item $\alpha^t = 1/t$.
\item $\alpha^t = 1/\sqrt{t}$.
\item $\alpha^t = 1$.
}
\ans{
    Option c
}
\item We discussed ``global'' vs. ``local'' features for e-mail classification. What is an advantge of using global features, and what is advantage of using local features? \\
\ans{
    Global features lead you to a more robust model, whereas local features can result in lower losses.
}
}


\end{document}